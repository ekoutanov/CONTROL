Chapter 4. Foundational Principles
---
At the heart of every software development method lies a bedrock of core principles. These "pillars" support the entire development process, shaping how developers approach problems, collaborate, and ultimately deliver the software. This chapter explores the foundational principles that underpin CONTROL in its entirety.

Every element of CONTROL is traceable to the foundational principles. So, one can be reasonably certain that if something appears in CONTROL, then some precursor to it is described here.

# Personal accountability
The word "accountability" will appear hundreds of times. It might've already caused some anxiety. The reader might have equated accountability to being responsible for something important. Or imagined accountability as being fired when you drop the ball. So, it's time we pause to understand what it really means.

You know you're responsible for something when you dig up your job description in the HR archive. If you're a mid-level software engineer, it might resemble the following<sup>1</sup>:

* Write efficient and well-documented code.
* Collaborate with senior developers to develop and maintain software applications.
* Engage with fellow teammates on continuous improvement initiatives.
* Work with stakeholders to determine user requirements.
* Review current developments to keep abreast of technology trends.
* ...

><sup>1 </sup>Taken from an actual job ad.

_Responsibility refers to the obligation to perform some task or duty._ It's assignable. Many people might be collectively responsible for the same task. A team, for example, may be responsible for delivering a new application. The job description above lists the things that have been assigned to you either directly or through association with your team.

Does any of that make you accountable?

Accountability implies answerability for a task or duty. It cannot be assigned; it must be accepted. However, one may accept accountability implicitly and even unintentionally by entering into an agreement. When starting a new job, for example, you will be accountable for certain outcomes, despite the job description lacking a separate "accountabilities" section. As a paid professional, you are expected to know where your accountability lies, which sadly may not be obvious to many.

The assignability point is crucial. While responsibility may be delegated, at least in part, accountability cannot be transferred by the assignee without agreement from the assigner. Consider an example. Shelley, the senior engineer in charge of information security at Bookworm Inc. does not have to conduct the penetration test herself before launching the new online marketplace — she can delegate to an external agency. But if the system gets compromised, she'll have to explain what went wrong. She cannot point the finger at the third party, even if they were grossly negligent in fulfilling their responsibilities.

Accountability is often viewed in a negative light, but it has a reciprocal upside. Suppose a wave of ruthless cyberattacks hits online bookstores, leaving only Bookworm Inc. impervious and raking in revenue — its own and the competitors'. Who do you think will get the credit?

Accountability is typically assigned to individuals. It cannot be assigned to a responsibility over which the individual has little or no control.

_Accountability is an assurance that an individual is evaluated on their performance related to something for which they were responsible._ 

There are several key ingredients of accountability that we can now discuss in the context of the Bookworm Inc. example:

1. **The person must know what's expected of them.** Accountability must be accepted and it's unreasonable to expect something of Shelley if she doesn't fully comprehend that expectation. She's not some abstract engineer, but one focused on information security. This must be made explicit in her role description. Ideally, an accountabilities section should be drawn up.
2. **Must be uniquely attributable to them.** Something that may be expected of Shelley may also be expected of someone else. If there are two (or more) engineers responsible for security, might each one reasonably assume that this responsibility will be carried out by their colleague? It must be made clear that no one else is expected to perform this duty — hence Shelley is _in charge of_ security.
3. **Must be within their control.** Does Shelley have the ability to perform her duties? Is she adequately skilled and resourced? In other words, can she control the outcome of the security assessment process? Note that we are not referring to being in control of the cyberattack but the measures that Bookworm Inc. could have reasonably taken to respond. Here's the kicker: even if Shelley accepts accountability for a task or duty she is incapable of carrying out, and her manager was reasonably aware of that, it cannot be later used against her. We often legitimately assign tasks to people beyond their ability — that's part of training and life in general. But we generally have a plan B in place, and we don't hold them to account. It's called "taking advantage" otherwise.
4. **Must be measurable.** Can Shelley's performance be assessed? This may not be as simple as a binary "Yes, we were unaffected by an attack" or "No, we got clobbered". In practice, many cyberattacks are occurring routinely; some may be targeted and others opportunistic. Distributed Denial of Service (DDoS) attacks may slow the website down only slightly, or cripple it entirely, depending on the provisioned DDoS mitigation capacity. You might measure the availability of critical revenue-generation capabilities and attribute downtime to cyberattacks. That's not enough, however. Bookworm Inc.'s performance should ideally be compared against an industry benchmark. If Shelley is going to be pinned to some benchmark, she must be aware of it and given a fair opportunity to top it. Crucially, one cannot assign a penalty to one person and a reward to another for the same task or duty; not only would it contradict point 2 but foster a culture of blame.

Returning to the original "job description" example, it should be fairly obvious where the accountability lies. It's almost entirely concentrated in the first bullet point. Peer reviews aside, writing code is mostly an individual activity. The code should be efficient and well-documented. These things are measurable even if there might be some subjectivity in the assessment. The second bullet point reveals that there will be senior engineers involved, so we roughly know who might be doing the measuring and where the subjectivity might come from. Committed code quality is entirely within the engineer's control, which we know is essential to accountability. Finally, they may be better or worse than average coders by some metric; the latter may be derived internally by observing other engineers in the organisation.

The rest of the points are mostly fluff. Collaboration requires multiple parties. Engagement with fellow teammates is another team activity. It's not uniquely attributable to the employee. 

Working with stakeholders to elicit requirements seems like a clean pin at first glance, but it doesn't hold water on closer inspection. Sure, everyone ought to work with the business to make the best possible product — it's in everyone's best interest. Would you, for example make your mid-level engineer who has recently joined, likely with not a sausage of domain experience, accountable for getting requirements right? Even suppose they mastered this task; the credit would still be taken by the product team! Recall, the responsibility must be uniquely attributable.

The final bullet point — reviewing current developments and keeping abreast of technology — is uniquely attributable, under the engineer's control and very much known to them. But it's unaccountable because it cannot be quantified in isolation — it is just a means to an end. That end is the code that is to be produced, and its qualities, in turn.

Accountability is hierarchically composable: a line manager is implicitly accountable for the performance of their reports. By straightforward extension, the manager is accountable for the aggregation of the outcomes under their remit. Technically, the process flows in reverse: the manager accepts accountability and then delegates associated responsibilities to their direct reports, refining and cementing individual accountabilities in the process. Stated otherwise, hierarchical accountability involves superiors delegating authority to subordinates, who in turn are accountable to their superiors for their actions and decisions.

A well-designed accountability model goes beyond simply tracking performance metrics. It strikes a balance between holding individuals accountable for their work and ensuring they have the resources and support to succeed:

* **Safeguarding individual rights**:
  + **Clarity and fairness**: A good accountability model clearly defines responsibilities and expectations. This transparency prevents confusion and ensures everyone understands what's expected of them. 
  + **Focus on growth, not just blame**: The model should emphasise development and improvement over solely identifying blame for shortcomings. Feedback should be constructive and actionable.
  + **Due process**: The model should outline clear procedures for addressing performance issues. This ensures fairness and protects employees from arbitrary actions.
  + **Psychological safety**: Feeling safe to take risks, admit mistakes, and ask for help is crucial. The model should create an environment where open communication and learning are encouraged.
* **Ensuring resources for success**:
  + **Skills and training**: The model should identify skills and training needs required for individuals to meet expectations. Organisations should provide opportunities for skills development.
  + **Clear targets and goals**: Performance expectations should be SMART (specific, measurable, achievable, relevant, and time-bound). This ensures goals are clear, attainable, and aligned with objectives.
  + **Supportive environment**: Managers should provide ongoing support, coaching, and feedback to help employees overcome challenges and achieve their goals.
  + **The right tools and resources**: Employees need the necessary tools, technology, and resources to perform their jobs effectively. The accountability model should help identify and address any resource gaps. 

Benefits of this balance:

* **Increased motivation**: Employees are more motivated to achieve goals when they have the tools and support they need to succeed.
* **Enhanced ownership**: Clear expectations foster a sense of ownership and accountability for results.
* **Improved performance**: By focusing on development and providing support, the accountability model can lead to improved individual and hence overall team performance.
* **Reduced turnover**: When employees feel valued and supported, they're more likely to stay.

CONTROL is performance-orientated at its core and **accountability is the main mechanism by which performance is achieved**. Accountability is not an outright carrot or stick; it lays a spectrum within which a team member may perform. Some may be towards the carrot end, others towards the stick end, and many will loiter in between. Memorise this one phrase: _you cannot have accountability without countability_. It's a measure, first and foremost. And things that cannot be measured cannot be improved.

The reality of the modern workforce is that there are far more software engineering vacancies than competent engineers able to fill them within the salary ranges on offer. The increasing gap between supply and demand for skilled practitioners implies that an engineering organisation cannot exclusively hire absolute superstars; people are often hired on the prospect that they will grow into the role with adequate training and time. Likewise, the organisation may be limited in how many people can be laid off; both for legal reasons as well as the sheer volume of work that needs tending to. However, every engineering organisation must be able to accurately access individual performance, identify gaps and resourcing shortfalls, and create reward structures and career pathways to optimise performance in the long run. CONTROL is intentionally organised in ways that highlight and support individual achievements, while discouraging and ultimately weeding out underperformance.

Identifying essential accountabilities among the seemingly unbounded responsibilities of a modern-day engineer is perhaps the reason that engineering organisations routinely underperform. But does it have to be so hard? Would it hurt the employer to explicitly enumerate the accountabilities? This question is perplexing because the rational answers don't make much sense. It is easy enough to list accountabilities. There aren't that many of them. Listing them benefits both the employer and the employee. What is the problem then? Maybe they are concerned that employees could misinterpret accountability as a ploy to work them harder. Or is it, perhaps, that the culture of engineering underperformance has become so prevalent that management is unwilling to recognise accountability for fear that it might return to haunt them?

# Context preservation
The breaking down of a complex problem into tiny bite-sized chunks so that developers of average skill can build the pieces is, by all accounts, a remarkable achievement. This practice is virtually inscribed in Agile and quickly perfected through everyday practice. Once a work ticket has been created, complete with its user story and acceptance criteria, the knowledge (or lack thereof) of the original problem becomes inconsequential to implementation. The ticket _is the problem_, as far as its assignee should be concerned.

Many engineering texts have been published on the topic of providing maximum context to engineering teams and empowering them to make optimal decisions. One such book was written by a friend and former colleague, Dion Beetson (2000). Dion and I have worked shoulder-to-shoulder on large projects in two companies and we have come to the joint understanding that many causes of underperformance are due not to poor solutions but to poor (articulation of) problems. We are, of course, talking about context.

Context plays a pivotal role in all problem-solving with activities such as reasoning and interpretation, even the most mundane ones. When a human brain decodes an image of what appears to be another human in front of them, their reaction might vary from engaging in social interaction — if they are at a party, or slamming the brakes — if they are behind the wheel. So, context clearly influences reasoning. Yet, context in itself does not uniquely determine the solution to the problem. There might not even be a problem to speak of. And context probably cannot refer to the complete environment, as most of it is immaterial to the reasoning.

While the definition of context seems both intuitive and elusive, we should nonetheless attempt to define it. The most useful definition that I could find is that "context is what constrains problem-solving without intervening in it explicitly", appearing in an early survey of contextual reasoning in Artificial Intelligence (Brézillon, 1999).

Context preservation leads to more informed decision-making through —

* **Holistic understanding**: Better decisions can be made when the full scope of the problem is understood, including its relevant history<sup>2</sup>, contribution to the whole, material constraints, and other related factors. This holistic understanding helps avoid wasting efforts on solutions that do not fit the real-world problem or must be reworked substantially down the track. Engineering problems often have multiple solutions, the vast majority of which are suboptimal in some way. Enriching problem definitions with the right context helps reduce the likelihood of a poor solution emerging. It cannot guarantee the best solution is chosen but removing context almost always ensures the opposite.
* **Traceability**: Keeping context allows for the traceability of decisions and resulting changes. In other words, for every change that was made, we have a record of how that change served the broader purpose of the system. This is particularly important in complex projects where decisions can have far-reaching implications. Improving traceability through context also promotes the retention of knowledge and streamlines staff onboarding. It also assists longer-term maintenance, wherein changes are considered to aspects of the system that may have been in production for considerable time by engineers who were not involved in its construction. The ability to reconstitute the original context may be more significant to the outcome than the skills of the engineers or the diligence of the testers.

><sup>2 </sup>It is chiefly for this reason that I have chosen to dedicate the first two chapters to the analysis of the problem. I don't expect the reader to follow my instructions vacuously. But I also don't expect the reader to challenge my instructions without understanding the full context within which they have come to be.

Agile's approach to spoon-feeding problems is a triumph of short-term gains over long-term value production. As the antidote, CONTROL strives to offer the complete context to a cross-functional delivery squad, which is fully empowered to come up with a solution, within _a priori_ constraints. These are shaped not only by the business through a set of _abstract requirements_, but also by the Architecture community. The latter is through a combination of conceptual views produced during the initial analysis (in the _Ideation_ and _Abstraction_ phases), as well as robust architectural principles. Many stakeholders may thus contribute to constraining the solution space without erasing the problem context. (We cover abstract requirements and the phases of the delivery lifecycle in Chapter 5.)

In practical terms, context is preserved through —

* **Documentation**: Ranging from the comprehensive description of the original problem (potentially augmented with worked examples, references to competitor sites, mock-ups, etc.) to the individual records of design decisions. This includes documenting assumptions, constraints, and the rationale behind decisions, which can be retraced to the elements of the original problem.
* **Knowledge management systems**: Used to organise documents, manage historical versions, limit access, and enable documentation to be easily searched. 
* **Linking**: Knowledge should not be siloed in documentation repositories. Traceability should be cemented through bidirectional links between the short-lived work tickets and the long-lived documentation. It should be easy for anyone working on a solution to trace back to the original problem statement. Likewise, any problem solved by the delivery organisation should be mappable to the specific work items (and hence software, infrastructure or configuration changes) that were required. Modern knowledge management systems and project management tools, such as Confluence and Jira, respectively, are easily capable of this.
* **Direct and continuous communication**: Ongoing communication between the business (proxied or facilitated by the Product Owner), Architecture, and the delivery squad.
* **Tacit knowledge**: Concentrating large amounts of knowledge in people's minds is generally considered undesirable due to both inaccessibility (hard to express and extract) and the obvious key personnel risk. On the flip side, tacit knowledge is not a problem in and of itself; it only becomes problematic when this knowledge is not "backed up", in a manner of speaking, to more durable and accessible media.

# Conceptual integrity
What would the Sydney Opera House look like if its exterior was designed by a committee of twelve architects?

The decentralisation of design decisions was popularised by Agile methods as an effective way of removing key decision-makers from the critical path of a software project's delivery. The resulting architecture may resemble a tapestry stitched from independent and often contradictory perspectives that lack cohesion and consistency. We must consider the problems with this approach.

In 1975, Fredrick Brooks in his "Mythical Mon-Month: Essays on Software Engineering" famously said, "I will contend that conceptual integrity is the most important consideration in system design. It is better to have a system omit certain anomalous features and improvements, but to reflect one set of design ideas, than to have one that contains many good but independent and uncoordinated ideas." Brooks was, of course, speaking from the perspective of a software engineer, not a project manager. And experienced engineers realise that building software is only the tip of the iceberg; most of the effort will be expended in its maintenance.

There is a clear up-front cost to producing a conceptually sound solution when there is a relatively small group of skilled and tightly aligned experts available to complete the work. Whether slowing down during this early juncture to produce an integral solution is economically worthwhile might be answered differently depending on the organisational context. A garage startup, for example, would prefer expediency over any long-term consideration, as its future is predicated mostly on factors outside its control and cannot be reasonably planned for. Similarly, a throw-away prototype has little to gain from decision foresight. On the other hand, an established commercial organisation, having failed to properly consider long-term consequences of hastily made decisions, may inadvertently consign itself to a path to eventual irrelevance, as more savvy competitors may have invested in more sustainable foundations in the meantime.

CONTROL does not profess a one-size-fits-all approach and intentionally does not cater to garage startups or throw-away software. It focuses on mainstream software engineering, where the initial expense of prudent technology decisions is easily amortised over the lifetime of the resulting intellectual assets, taking into consideration knowledge retention, staff churn, and maintenance obligations. Specifically, we consider conceptual integrity to play a pivotal role in the success of software projects, owing to the following benefits:

* **Approachability**: A well-defined and integral conceptual foundation makes the codebase easier to understand, maintain, and extend. Engineers who know some parts of the system will quickly absorb other parts, as many of the underlying concepts will immediately feel familiar and approachable. It also reduces the reliance on documentation and walkthroughs as onboarding aids.
* **Survivability**: A smaller set of unique concepts simplifies the overall architecture, making it easier to extend in the same vein. It is less likely that future maintainers will be tempted to introduce foreign concepts into an integral design, without being challenged to produce sound justification of their benefits, which must outweigh the dilution of conceptual integrity. Conversely, it is easy to introduce poorly conceived and largely unscrutinised concepts into an already chaotic and kludgy design. In other words, if we are to assume that any architecture will invariably change over time, the architectures embodying weaker conceptual integrity will be subjected to a disproportionately greater change.

![Sydney Opera House construction](images/opera-house-construction.jpeg)

_The construction of the Sydney Opera House in 1965._ (Source: Hornibrook, 2021)

> In discussing conceptual integrity, one cannot help but draw an analogy to the Broken Windows Theory in criminology. The theory was introduced to the general public in the March 1982 issue of the Atlantic Monthly by social scientists James Wilson and George Kelling. The article offers a radical perspective on policing. According to the authors, a concerted focus on curtailing minor civil disorder, such as vandalism, loitering, public drinking and fare evasion, will reduce more serious crime. In 1985, the New York City Transit Authority hired Kelling as a consultant. He later consulted to the Boston and the Los Angeles police departments.
> 
> Their theory was based on an earlier experiment by Philip Zimbardo 1969. I will summarise it here for your convenience.
>
> Zimbardo left two abandoned cars, without number plates and with their hoods opened, in two neighbourhoods: a poor one with mass unemployment and vandalism, and an affluent one with no prior history of vandalism. In the poor neighbourhood, the first attacks occurred within minutes. Within 24 hours of exposure, everything of value had been stripped from the vehicle. What was left was vandalised.
>
> Nothing remarkable happened to the abandoned car in the affluent neighbourhood. It stood unmolested for over a week, until Zimbardo himself took to its window with a hammer. The car's fate quickly followed its "poor" twin: it was looted and vandalised within hours.
>
> Critics argue that the experiment lacked proper controls and the methodology wasn't sufficiently rigorous to draw definitive conclusions. Regardless, the experiment appears to be easily reproducible (Keizer, Lindenberg, & Steg, 2008), although the reader is strongly discouraged from attempting it. It also corroborates the findings of Ringelmann (1913), suggesting that people will make comparisons with others and adjust behaviour accordingly: they will take on apathetic tendencies if they perceive that others have done the same or are likely to.
>
> The Broken Windows Theory is a sobering indication of how quickly a system can turn into junk once the first signs of neglect have been observed by those who were otherwise ready and willing to care for it.

In practical terms, conceptually integrity is achieved through —

* **Tightly aligned decision-makers**: A smallish group of experts entrusted with key technical decision-making throughout all stages of the system's evolution. 
  + The core group will typically comprise Architects and Principal Engineers but may involve subject matter experts in the decision-making process from time to time.
  + Group alignment and the ultimate integrity of the design is typically fostered by one, utterly credible individual, who needn't be directly involved in all decision-making but holds the power of veto over the group's decisions.
* **Early definition of foundational architecture**: An integral architecture cannot emerge circumstantially. It must be established early at a conceptual level, rationalised, and communicated clearly. 
  + It should outline the system's core principles, long-lived elements, and relationships between them.
  + Example: A system that is conceptually underpinned by Event-Driven Architecture (EDA) should clearly show a set of loosely coupled components communicating with and reacting to events. Many of the key components partaking in event-driven interactions will likely be known from the outset; otherwise, a reference model may be produced, acting as a template for future concrete architectures. The architectural viewpoints should be backed by a clear set of EDA principles for guiding the system's evolution. Finally, all decisions should be justified in the context of the system's objectives, present and future.
  + Don't think of reference models as purely a figment of enterprise architects' imaginations, conceived in their proverbial ivory towers. They are useful tools for abstraction and can be effective at the communication and comprehension of complex concepts, even if concrete models already exist. It takes great skill to produce viable reference models. But once there, they can be immensely useful.
* **Clear and principled decision-making**: The architectural principles outlined in the formative stages of the architecture must be refined over its evolution and referenced consistently and incessantly throughout all decision-making.
  + Transparency is important: It should be abundantly clear how every decision ties back to the foundational principles. These relationships should be apparent not only to the present decision team but to anyone in the delivery organisation. It should also be apparent to future contributors. Decision transparency is needed to garner trust and buy-in from all levels in the organisation.
  + If decision-makers systematically choose to neglect principles, they risk fostering an ad-hoc and unreproducible decision-making process and diluting the conceptual integrity of the system. Doing so also sets a bad example for the up-and-coming decision-makers within the delivery organisation.

> There is a leap from the architecture of the Sydney Opera House to the rapid degradation of a software architecture having incohesive design decisions at its outset. The former is an example of a conceptually integral design. Each of its magnificent roof shells is of precisely uniform curvature, not that it was strictly required from an aesthetic or structural standpoint. But the architect insisted upon it. In adopting uniformity, the civil engineering team discovered a way to economically build the shells by casting the half-shells in a common mould as sections of a sphere, despite each shell being of a different size. Before this method was discovered, many had argued that the shells were impossible to make (Hornibrook, 2021).
>
> The Opera House has been treasured by Sydneysiders for over fifty years. It's hard to imagine anyone wanting to mess with it, for the design itself deters any such thought. And till now, there has been one record of vandalism — during an antiwar protest in 2003. Purposeful and elegant software architecture that embodies a unified set of concepts also encourages people to take care of it, so that it may be purposeful and elegant for longer.


# Chalk outlines
For each of CONTROL's foundational principles, some points benefit from a further dissection.

## Balancing the effects of personal accountability
It is worth remarking on both the negative (being blamed for failures) and positive (being credited for successes) aspects of accountability. In the former, accountability measures may impede risk-taking in both its good and bad forms. People may favour overly conservative approaches to avoid blame. In the latter, accountability leads to improved performance by rewarding achievements, which we know require controlled risk-taking. It might appear that accountability is a contradiction in and of itself.

How do we reconcile accountability? Several factors that influence whether an accountability model ends up promoting or stifling innovation:

* **Organisational culture**: Is it one of blame, where mistakes are harshly punished? Is there a lack of psychological safety, where employees do not feel safe to make mistakes or speak up? Organisations that celebrate achievements create a positive environment where employees feel motivated to excel. And a culture that supports risk-taking and learning from mistakes encourages employees to innovate and improve continuously.
* **Leadership style**: Is the leadership heavily authoritarian? Are there micromanagers who closely monitor every detail and hinder employees' ability to take initiative? Transformational leaders who inspire and motivate their teams, and who emphasise personal growth and development, can foster a sense of ownership and trust.
* **Performance management**: Do performance evaluations focus solely on outcomes without considering context or effort? Did employees receive constructive feedback on their actions? Performance measurement that considers both successes and the effort behind attempts can encourage risk-taking and innovation. Providing regular, constructive feedback helps employees understand their strengths and areas for improvement, fostering a growth mindset.
* **Communication and transparency**: Are employees confused about what's expected of them? Were certain decisions made on their behalf and without adequate consultation? When expectations and accountability standards are clearly communicated, employees understand what is required and can work towards those goals with confidence.

Ultimately, it is about the _balance of the upside and the downside_. Without an upside, innovation is impossible and progress is sluggish. Without a downside, underperformance cannot be identified and corrected. Managers need to understand that environmental factors have a profound effect on the effectiveness of accountability models. They must create an environment of positive balance, where the upsides perceptibly outweigh the downsides without entirely erasing the latter. This means encouraging risk-taking, showing trust and providing support, recognising achievement in the context of effort, and ensuring that accountability measures are fair, transparent, and consistently applied.u

I assert that accountability must be uniquely attributable to an individual and within their control, which could be taken with scepticism in environments where collaborative work and interdependencies are prevalent. I do not believe the two viewpoints to be conflicting.

My viewpoint is not unique by any stretch. The need for personal attribution of accountability has received numerous support in literature (Robbins & Coulter, 2021, Covey, 2024, Lencioni, 2002, Katzenbach & Smith, 1993, and PMI, 2021). Many of these (Katzenbach & Smith, Lencioni, Robbins & Coulter), have specifically addressed the role of personal accountability in teamwork settings.

I have come to believe that much of the struggle that people experience in reconciling the notions of personal accountability and team performance is owed to the more fundamental confusion between accountability and responsibility. I have, for this reason, addressed their differences early in this chapter.

## On the preservation of context in Agile environments
Emphasising the importance of preserving context for holistic understanding contrasts with Agile's task-focused approach. This may be taken as unconditional criticism of Agile with respect to its problem-solving effectiveness. In the worst case, it may be seen as promoting anti-Agility.

I do not believe Agile to be a breeding ground for ineffective problem-solving in theory, nor that fine-grained task breakdown necessarily leads to unfitting solutions. I argue that the practical application of Agile methodology in the overwhelming majority of project contexts leads to precisely those effects.

In theory, practice and theory are alike. In practice, they differ. As I have stated in Chapter 2, there is a chasmic rift between the ideal use of Agile (as instructed by its founders and embodied in their theories) and the reality that stands before us today. Agile is routinely applied to problem contexts entirely outside its competency. In those instances where bite-sized tasks are randomly diffused across a large and dispersed engineering cohort, where each engineer is left to solving a micro-problem, one cannot expect context to propagate effectively.

## Broken windows as a metaphor for software maintenance
Drawing an analogy between the Broken Windows Theory in criminology and software systems development might be seen as an oversimplification and thus unreasonable comparison. Some might argue that software maintenance issues are complex and cannot be compared to acts of vandalism.

I have personally argued against the use of analogy from unrelated contexts as means for swaying a receptive audience. In our case, software development is, indeed, a complex activity. It is nonetheless carried out by people. There is nothing to suggest that software engineers possess traits that make them immune to the sorts of problems highlighted by the Broken Windows Theory. Consider the parallels:

* **Accumulation of problems**: In both contexts, small issues can snowball into larger problems if left unaddressed. Just as a neglected broken window can accelerate the weathering of a building's interior, ignoring minor software issues (like poor code quality, poor test coverage, or outdated dependencies) leads to the accumulation of technical debt.
* **Root cause amplification**: Criminology suggests that visible signs of disorder can encourage further disorder. Similarly, in software development, a neglected codebase can influence developers to be less diligent in their coding practices, leading to a decline in overall software quality. Equivalently, an initially incohesive software design relieves expectations of future design cohesion.

Imagine you were tasked to make changes to two Java classes: one had comprehensive unit test coverage and the other had none. With all the professionalism and good will at your back, would you expend equal efforts in writing the unit tests for both classes? Would your colleagues?

# Summary
CONTROL derives from a small set of foundational principles. Nothing in CONTROL may exist unless it is directly supported by at least one of the foundational principles and does not contradict the others.

The first principle — **personal accountability** — is often misunderstood, even among seasoned practitioners. Accountability is confused with responsibility; the former is not merely about performing a series of assignments but involves being answerable for the outcomes. Responsibility can be delegated and shared among team members, whereas accountability is personal and must be consciously accepted.

Accountability must be clear, uniquely attributable, within the individual's control, and measurable. Accountability also composes hierarchically: managers delegate responsibilities and assign specific accountabilities to their direct reports, while retaining accountability for overall performance.

A well-designed accountability model should balance holding individuals accountable with providing necessary resources and support, ensuring clarity, fairness, and psychological safety. Such a model fosters motivation, ownership, and improved performance, reducing turnover.

The second principle — **context preservation** — ensures that people are provided a holistic understanding of the problem, including its relevant history, constraints, and impact on the bigger picture.

While Agile methodologies excel at simplifying tasks for implementation, they often strip away essential context in the process. This can lead to potential misunderstandings and unfit solutions requiring rework.

Context awareness helps avoid uninformed and thus suboptimal solutions and wasted effort. By keeping track of decisions and their justifications within the problem context, future engineers can understand the rationale behind legacy decisions and make informed changes. This is essential for complex projects and long-term maintenance.

The final principle — **conceptual integrity** — underscores the importance of an uncompromising, cohesive, and consistent design at the outset of a software system.

Practical measures to achieve conceptual integrity include involving a small, tightly aligned group of decision-makers, defining foundational architecture and cornerstone principles early, and maintaining clear, principled decision-making throughout the system's evolution. The Broken Windows Theory is a pertinent illustration of how even minor neglect can lead to a rapid decline in system integrity. By prioritising conceptual integrity, CONTROL creates software that is maintainable, adaptable, and has a long lifespan. _Integritas super omnia._